---
title: "Fuzzy Logic Induced Content-Based Recommendation"
author: "Nguyen Quy Khoi"
date: "June 01, 2023"
geometry: "left=1in,right=0.5in,top=1in,bottom=1in"
output:
  pdf_document:
    highlight: tango
    #latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
fontsize: 12pt
fontfamily: "newtx"
urlcolor: blue
---

```{r setup, include=FALSE, results='hide'}
# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")
n = 10
# set a new output hook to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      # truncate the output
      x <- c(head(x, n*0.7), '....', tail(x, n*0.3 + 1))
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})
```

\newpage

# Introduction to content-based recommendation

When a friend comes to you for a movie recommendation, you don't arbitrarily start shooting movie names. You try to suggest movies while keeping in mind your friend's tastes. Content-based recommendation systems, also known as content-based filtering methods, try to mimic the exact same process.

Consider a scenario in which a user is browsing through a list of products. Given a set of products and the associated product properties, when a person views a particular product, content-based recommendation systems can generate a subset of products with similar properties to the one currently being viewed by the user.

The following example demonstrates how content-based recommendation systems work.

## Introduction to wine suggestion system

An example of using content-based recommendation is wine suggestion. We will use the wine data set from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/wine). This data set is the result of the chemical analysis of wine grown in the same region in Italy. We have data from three different [cultivars](https://en.wikipedia.org/wiki/Cultivar) (from an assemblage of plants selected for desirable characters).

In the data set, rows represent each wine brand, while the columns represent the properties of the wines. The table is extracted from UCI machine learning repository as follows:

```{r, message=FALSE}
# data.table provides a high-performance version of base R's data.frame with 
# syntax and feature enhancements for ease of use, convenience and
# programming speed.
library(data.table)
library(here); library(skimr); library(janitor)
# differences between "<-" and "="
# "<-" is for assignment, standard everywhere
# "=" is for passing arguments, (allowed only for top level env)
#wine.data <- fread('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')
wine.data <- fread('data\\wine.data')
skim_without_charts(as.data.frame(wine.data))
```

## Chemical properties similarity between two wines

We have a total of 14 columns. The first column `V1` represents the cultivar, which is assigned in `wine.type`. The remaining columns are the chemical properties of the wine, which are stored in `wine.features`:

```{r}
wine.type <- wine.data[,1]
wine.features <- wine.data[,-1]
# scale feature properties such that each column has mean=0 and std=1
wine.features.scaled <- scale(wine.features)
# convert feature properties into matrix to perform linear algebra operations
wine.mat <- data.matrix(wine.features.scaled)
# assign row names for matrix wine.mat (name = sequence from 1 to 178)
rownames(wine.mat) <- seq(1:dim(wine.features.scaled)[1])
# transpose the matrix, feature properties are row, wines are column
# to perform pearson coefficient calculation between 2 columns
wine.mat <- t(wine.mat)
```

The pairwise correlation in the numerical matrix is measured using Pearson coefficient after scaling. In this problem, the Pearson coefficient matrix returns *the similarity between 2 wines* (i.e., compare 2 columns).

The output is the similarity matrix, which shows how closely related items are. The values range from -1 for perfect negative correlation, when two items have attributes that move in opposite directions, and +1 for perfect positive correlation, when attributes for the two items move in the same direction. The diagonal values will be +1, as we are comparing a wine to itself.

```{r}
# perform pearson correlation calculation, which returns an 178x178 matrix
cor.matrix <- cor(wine.mat, use="pairwise.complete.obs", method="pearson")
```

## Test case: Find top 4 similar wines to wine `3`

In this problem, the similarity matrix is useful for finding similar wines (target column) in the wine's catalog. For example, if a customer likes wine `3` and wants to know if the company's list of wines has something similar. Using the similarity matrix, we will look at the third row to find any wines having high correlation value (i.e., column value closer to 1). **The top 4 matches** of wine `3` in terms of chemical properties are wines `52`, `51`, `85`, and `15`. The chemical properties of the matching wines are shown below:

```{r}
sim.items <- cor.matrix[3,]
# find the closest match by sorting row 3 in decreasing order
sim.items.sorted <- sort(sim.items, decreasing=TRUE)
# take out top 5 matches
sim.items.name <- names(sim.items.sorted[2:5])
rbind(wine.data[as.double(sim.items.name),])
```

# Case study: News aggregator

## Problem statement

We have 413840 news articles from different publishers, retrieved from [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/359/news+aggregator) has different categories: technical, entertainment, etc. The problem is building a recommendation system for anonymous customers. The customers either are first time visitors or we have not recorded their interaction with our products.

```{r message=FALSE, out.line=n}
library(dplyr)
library(ggplot2)
library(lubridate)
library(sentimentr); library(sets); library(slam)
library(tidytext); library(tidyverse); library(tm)

data.population <- read_tsv('data\\NewsAggregatorDataset\\newsCorpora.csv', 
                   col_names=c('ID','TITLE','URL','PUBLISHER','CATEGORY',
                               'STORY','HOSTNAME','TIMESTAMP'),
                   col_types=cols(
                     ID       =col_integer(),
                     TITLE    =col_character(),
                     URL      =col_character(),
                     PUBLISHER=col_character(),
                     CATEGORY =col_character(),
                     STORY    =col_character(),
                     HOSTNAME =col_character(),
                     TIMESTAMP=col_double()))

num.articles = 10 # number of related articles to recommend to the user
# sample without replacement 40,000 random data in the whole data set
index.sampled <- sample(1:nrow(data.population), 40000, replace=FALSE)
data.sample <- data.population[index.sampled,]
```

Every article has the following columns:

-   `ID`: A unique identifier.
-   `TITLE`: The title of the article (free text).
-   `URL`: The article's URL.
-   `PUBLISHER`: Publisher of the article.
-   `CATEGORY`: Categorization under which the articles are grouped (`b` = business, `t` = science and technology, `e` = entertainment, `m` = health).
-   `STORY`: An ID for the group of stories the article belongs to.
-   `HOSTNAME`: Hostname of the URL.
-   `TIMESTAMP`: Approximate time the news was published, as the number of milliseconds since the epoch 00:00:00 GMT, January 1, 1970.

## Exploratory Data Analysis

Firstly, the articles are grouped by category (as specified by `CATEGORY`) and arranged in a descending order. From observation, entertainment has the highest number of published articles.

```{r}
ggplot(data.sample) +
  geom_bar(mapping=aes(x=CATEGORY, fill=factor(CATEGORY)),
      	   colour ='black',
      	   stat   ='count') +
  ggtitle("Number of articles by category") + 
  xlab("Article category") +
  ylab("Number of articles") + 
  scale_fill_discrete(name  ='Category',
                      breaks=c('e','b','t','m'),
                      labels=c("entertainment","business",
                               "science and technology","health")) +
  theme(plot.title=element_text(lineheight=1, face="bold"))
```

Secondly, we order the publishers in descending order by the number of published articles. `Reuters` publishes the most articles, followed by `Businessweek`. Then, all articles of the top 100 publishers are obtained from `newsCorpora.csv` to design content-based recommendation engine. In this problem, a user chooses a random article in the top 100 publishers sampled data set `data.subset`.

```{r, message=FALSE, out.lines=n}
publisher.top <- data.sample |> group_by(PUBLISHER) |>
        summarise(count=n()) |> arrange(desc(count), PUBLISHER)
publisher.top

# get top 100 publishers, join with article tables
data.subset <- inner_join(data.frame(publisher.top[1:100,]), data.sample)
data.subset$count = NULL
```

## Design procedure of Content-based recommendation engine design
The problem statement from customer requirements is: **When a customer browses a particular article, what other articles should we suggest to him?** From the statement, some properties of the news articles must be analyzed to provide a similar content to the one the user is reading. Those properties are:

-   The article's content.
-   The article's publisher.
-   The article's category.

We are going to design our content-based recommendation engine in three steps, shown as follows:

1.  Calculate **cosine distance**. The bag-of-words model is constructed from the sampled article data set. Then, a similarity index is created using the vector space model.
2.  Search for relevant news articles in the similarity index. The recommendation engine first retrieves the top `r num.articles` related articles, sorted in descending order. It is calculated based on the similarity matrix developed in the previous step. For those `r num.articles` articles, we further calculate more features of the polarity of the articles. Then, two other similarity index parameters are the Manhattan distance and the Jaccard index. The Manhattan distance finds the absolute difference between the given article's polarity value and the top `r num.articles` articles, then normalized through min-max normalization. The Jaccard index finds the similarity of the target article and the top `r num.articles` articles based on publisher and category.
3.  Implement a fuzzy ranking engine. The ranking engine finds the top `r num.articles` matches in a ranked order using the cosine distance, Jaccard distance, and Manhattan distance.

## Design process of Content-based recommendation engine design

### Building similarity index

The code below creates a bag-of-words model using document term matrix representation. The model employs pre-processed documents of the sample `data.subset` to build a corpus.

```{r, out.lines=n}
# identify articles by their ID
corpus <- data.subset[,c('ID','TITLE')] |> rename(c(doc_id='ID',
                                                    text='TITLE')) |>
  DataframeSource() |> Corpus() |> 
  # text transformation in 5 steps below
  tm_map(removePunctuation)            |>
  tm_map(removeNumbers)                |> 
  tm_map(stripWhitespace)              |>
  tm_map(content_transformer(tolower)) |>
  tm_map(removeWords, stopwords("english"))

dtm <- DocumentTermMatrix(corpus, control=list(wordLength=c(3,20),
                                               weighting="weightTfIdf"))
inspect(dtm)
```

The articles list separated into two data frames: `title.df` and `others.df`. Data frame `title.df` stores article titles while `others.df` stores articles' ID, publisher, and category. Stemming algorithms are excluded in this problem. The documents are pre-processed in five steps. The text transformation process is summarized below:

1.  `removePunctuation`: remove punctuation

2.  `removeNumbers`: remove numbers

3.  `stripWhitespace`: remove unnecessary white spaces

4.  `content_transformer`: change words to upper or lower case, depending on the inputs. In this problem, the text is converted to lower case with `tolower` argument. In this way, algorithms can treat "Dog" and "dog" the same.

5.  `stopwords`: remove common words in a language. Some examples of stop words in English content are *the*, *is*, *at*, *which*, *and*, *on*, etc.

The code below performs two tasks: calculate cosine similarity, stored in `sim.score` and randomly choose an article from the sample `data.subset`. The elements in `sim.score` matrix correspond to the cosine similarity between two documents.

```{r, out.lines=n}
# calculate cosine similarity
sim.score <- tcrossprod_simple_triplet_matrix(dtm) /
  (sqrt(row_sums(dtm^2) %*% t(row_sums(dtm^2))))

# randomly choose an article from top 100 publisher sampled data set
user.read <- sim.score |> colnames() |> data.frame() |> sample_n(1)
user.article <- data.subset[data.subset$ID==user.read[1,1],]
t(user.article)
```

The output of the code represents the article a user choose. In this problem, the user decided to read **`r user.article$TITLE`** by **`r user.article$PUBLISHER`**. The recommendation engine is responsible for selecting top `r num.articles` similar articles to the article.

The calculating method for similarity index depends on the problem and objectives of the problem. The reasons cosine similarity is preferred compared to Pearson coefficient are:

-   Cosine similarity is invariant to changes in magnitude of values since it only consider frequency of the words. This is not the case for Pearson coefficient.
-   Cosine similarity neglects word order while Pearson coefficient is not. Thus, cosine similarity is suitable for *high-dimensional data* like words in a corpus.
-   Cosine similarity is more robust to outliers or noise than Pearson's coefficient.

### Ranking search results

For this step, the code below extract the IDs of top related articles to **`r user.article$TITLE`**, excluding the article itself.

```{r, out.line=n}
match.docs  <- sim.score[user.read[1,1],]
match.df    <- data.frame(ID              =names(match.docs), 
                          cosine          =match.docs,
                          stringsAsFactors=FALSE)
match.df$ID <- as.integer(match.df$ID)
# organize in descending order, take the top articles, excluding the original 
match.refined <- match.df |> arrange(desc(cosine)) |> head(num.articles+1)
```

Since the ID of article `r user.article$TITLE` is `r user.article$ID`, row `r user.article$ID` of `sim.score` is selected. The row is organized in descending order. Then, only `r num.articles` articles with the highest similarity index is are chosen (excluding the original article).

The code below calculates the sentiment score of the top `r num.articles` articles. Then, the scores are updated in column `polarity` of data frame `match.refined`.

```{r, out.line=n}
# inner join with data.subset to include all information about the articles
match.refined <- match.refined |> inner_join(data.subset)
# calculate sentiment score of each sentence in top 10 articles
# using their titles
sentiment.score <- sentiment(match.refined$TITLE)       |> 
                   group_by(element_id)                 |>
                   summarise(sentiment=mean(sentiment))
match.refined$polarity <- sentiment.score$sentiment
as_tibble(match.refined)
```

The function `sentiment` calculates the sentiment score of the top `r num.articles` related articles based on their titles. Since the calculation applies to each sentence, if an article title has more than one sentence, it may have two sentiment scores and above. Thus, the mean value is introduced to evaluate the overall score of each title.

The code below calculates the average value of publisher and category Jaccards index. We only concern if the publisher and category of related articles match `r user.article$PUBLISHER` and `r user.article$CATEGORY`, respectively. Thus, the final Jaccard index are the average value of two binary outputs.

```{r, out.line=n}
# Jaccard index of publisher
match.refined$is.publisher <- as.numeric(match.refined$PUBLISHER==
                                           user.article$PUBLISHER)
# Jaccard index of category
match.refined$is.category  <- as.numeric(match.refined$CATEGORY==
                                           user.article$CATEGORY)
# average Jaccard value
match.refined$jaccard <- (match.refined$is.publisher + match.refined$is.category)/2
```

The code below calculates the normalized absolute difference between the target article and related articles:

```{r}
# absolute difference (Manhattan distance)
match.refined$polaritydiff <- abs(match.refined[1,]$polarity -
                                    match.refined$polarity)
# min-max normalization
f_rescaling <- function(x){(x-min(x)) / (max(x)-min(x))}
match.refined$polaritydiff <- f_rescaling(match.refined$polaritydiff)

# clean up
match.refined$is.publisher = NULL
match.refined$is.category  = NULL
match.refined$polarity     = NULL
match.refined$sentiment    = NULL
match.refined$STORY        = NULL
match.refined$HOSTNAME     = NULL
match.refined$URL          = NULL
tibble(match.refined)
```

After calculation, all unnecessary columns are removed to rank the related articles.

### Ranking Search Results using Fuzzy Logic

In this problem, the fuzzy logic inputs are cosine, Jaccard, and polarity scores, while the output is the articles' ranking. We first convert them to linguistic variables. For example, let us take our cosine score:

-   `vlow = 0.2` (a very low score)
-   `low = 0.4` (a low score)
-   `medium = 0.6`
-   `high = 0.8`

We then define a membership function that is responsible for constructing fuzzy membership. Since there are no mandates for membership function selection, a cone with a radius of 0.2 is the chosen membership function. If the cosine similarity passes through the membership function, it becomes a linguistic value. The procedure also applies to the polarity score and Jaccard index.

Finally, a linguistic output variable called ranking score is defined. In the same way as the three inputs, it also has a range and a membership function. A simple rule can be as follows:

-   if `cosine score = vlow`, then `ranking = low`.

The code below defines fuzzy logic to rank suggested articles based on three similarity parameters.

```{r}
  #### Fuzzy Logic ####
sets_options("universe", seq(from=0, to=1, by=0.1))
cone.radius = 0.2
variables <- set(
  cosine=
  fuzzy_partition(varnames=c(vlow=0.2, low=0.4, medium=0.6, high=0.8),
                      FUN=fuzzy_cone, radius=cone.radius),
  jaccard=
  fuzzy_partition(varnames=c(close=1.0, halfway=0.5, far=0.0),
                      FUN=fuzzy_cone, radius=cone.radius*2),
  polarity=
  fuzzy_partition(varnames=c(same=0.0, similar=0.3, close=0.5, away=0.7),
                      FUN=fuzzy_cone, radius=cone.radius),
  ranking =
  fuzzy_partition(varnames=c(H=1.0, MED=0.7, M=0.5, L=0.3),
                      FUN=fuzzy_cone, radius=cone.radius)
  )

rules <- set(
  #### Fuzzy Rule ####
  # Low Ranking Rules
  fuzzy_rule(cosine %is% vlow, 
               ranking %is% L),
  fuzzy_rule(cosine %is% low || jaccard %is% far || polarity %is% away,
               ranking %is% L),
  fuzzy_rule(cosine %is% low || jaccard %is% halfway || polarity %is% away,
               ranking %is% L),
  fuzzy_rule(cosine %is% low || jaccard %is% halfway || polarity %is% close,
               ranking %is% L),
  fuzzy_rule(cosine %is% low || jaccard %is% halfway || polarity %is% similar,
               ranking %is% L),
  fuzzy_rule(cosine %is% low || jaccard %is% halfway || polarity %is% same,
               ranking %is% L),
  fuzzy_rule(cosine %is% medium || jaccard %is% far || polarity %is% away,
               ranking %is% L),
    
  # Medium Ranking Rules
  fuzzy_rule(cosine %is% low || jaccard %is% close || polarity %is% same,
               ranking %is% M),
  fuzzy_rule(cosine %is% low && jaccard %is% close && polarity %is% similar,
               ranking %is% M),
    
  # Median Ranking Rules
  fuzzy_rule(cosine %is% medium && jaccard %is% close && polarity %is% same,
               ranking %is% MED),
  fuzzy_rule(cosine %is% medium && jaccard %is% halfway && polarity %is% same,
               ranking %is% MED),
  fuzzy_rule(cosine %is% medium && jaccard %is% close && polarity %is% similar,
               ranking %is% MED),
  fuzzy_rule(cosine%is% medium && jaccard %is% halfway && polarity %is% similar,
               ranking %is% MED),
  
  # High Ranking Rule
  fuzzy_rule(cosine %is% high,
               ranking %is% H)
  )

ranking.system <- fuzzy_system(variables, rules)
print(ranking.system)
plot(ranking.system)
```

We can define more complex rules involving multiple linguistic variables. After evaluating all the rules, we proceed to the inference process. The last step is the defuzzification of the above result into a numerical value. In this problem, the defuzzification technique is the centroid method. The membership plot at the end is responsible for the fuzzification process.

The code below ranks articles based on fuzzy logic with three parameters: cosine similarity, Jaccard index, normalized Manhattan distance (or polarity in this context). The output `match.final` is the suggested articles if a user chooses `r user.article$TITLE` from `r user.article$PUBLISHER`

```{r}

get.ranks <- function(dataframe){
  defuzzied_fi <- ranking.system |> 
    fuzzy_inference(list(cosine  =as.numeric(dataframe['cosine']),
                         jaccard =as.numeric(dataframe['jaccard']),
                         polarity=as.numeric(dataframe['polaritydiff']))) |>
    gset_defuzzify('centroid')
  return(defuzzied_fi)
  }

# apply get.ranks in every row of match.refined to get the fuzzy ranking
match.refined$ranking <- apply(match.refined, 1, get.ranks)
match.final <- match.refined |> 
  select(-one_of('jaccard','polaritydiff','cosine')) |>
  arrange(desc(ranking)) |>
  filter(ID!=user.article$ID)
match.final$ID = NULL
match.final$TIMESTAMP = as_datetime(match.final$TIMESTAMP/1e3)
as_tibble(match.final)
```

\newpage

# Appendix 1: Self-noted terminologies

## Sampling methods

There are two different ways to collect samples: **Sampling with replacement** and **sampling without replacement**.

-   **Sampling with replacement** is the method where the items in the samples are *independent* because the outcome of one random draw is not affected by the previous draw. Sampling with replacement is useful in many different scenarios in statistics and machine learning like *bootstrapping*, *bagging*, *boosting*, *random forests*, etc. The sampling method allows generating different models with the same data set. This is less time-consuming and expensive than acquiring new data every time a new model is built.

-   **Sampling without replacement** is the method where the items in the samples are *dependent* because the outcome of one random draw is affected by the previous draw. It is typically useful when we want to select a [random sample](https://www.statology.org/sampling-methods/) from a population.

## Similarity index

**Polarity of the document**. A subjective content about a topic tends to have a *positive*, *negative*, or *neutral* perspective. Polarity identification algorithms quantify the perspective using text mining. [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry) is a simple algorithm to measure the polarity value.

In data analysis, **cosine similarity** is a *measure of similarity* between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval $[-1,1]$. The term cosine distance is commonly used for the complement of cosine similarity in positive space, that is

$$
\text{cosine distance}=D_C(A,B):=1-S_C(A,B)=1-
\frac{\displaystyle\sum_{i=1}{n}(A_iB_i)}
{\displaystyle\sqrt{\sum_{i=1}^nA_i^2}\sqrt{\sum_{i=1}^nB_i^2}}
$$

The **Jaccard index**, also known as the **Jaccard similarity coefficient**, is a statistic used for gauging the [similarity](https://en.wikipedia.org/wiki/Similarity_measure) and [diversity](https://en.wikipedia.org/wiki/Diversity_index) of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets. The **Jaccard distance**, which measures *dis*similarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:

$$
d_J(A,B)=1-J(A,B)=\frac{|A\cup B| - |A\cap B|}{|A\cup B|}
$$

## Information retrieval model

In a **bag-of-words model**, every document is represented as a collection of words. In the model, the order of words is irrelevant. A good way to organize these bag-of-words models is using a **document term matrix**.

A **document term matrix** is a matrix representation of words in a collection of writings. For instance, we have 100 documents, which is called a *corpus*. In order to build the matrix, we first list out all unique words in 100 documents, which is called a *vocabulary*. If there are 5,000 unique words in 100 documents, the matrix's dimension is 100 x 5,000, where the rows are the documents and the columns are the words in the vocabulary.

Another example, considering a corpus of *sentences*:

1.  **Sentence 1**: Dog chased cat.
2.  **Sentence 2**: Dog hates cat.

The document term matrix represents the bag-of-words model of the 2 sentences, which is shown below:

| Document/word  | hates | dog | chased | cat |
|----------------|-------|-----|--------|-----|
| **Sentence 1** | 0     | 1   | 1      | 1   |
| **Sentence 2** | 1     | 1   | 0      | 1   |

**Term frequency** is the number of times a word appears in a document. For example, the term frequency of the word "dog" in sentence 1 above is 1, with a normalization of 1/3. Another example is an article that has 500 words, 25 of which mention the word "dog". The term frequency is 25, or 0.05 if it is normalized.

**Document frequency** is the number of documents in the corpus that contain a word. In the example above, the document frequency of the word "dog" in the corpus is 2.

**Inverse document frequency** (IDF) measures how important a word is to a document. The higher the IDF, the more rare and informative the word is.

$$
IDF(w)=\frac{N\displaystyle}{df(w)}
$$

**TFIDF** is the product of term frequency (TF) and inverse document frequency (IDF). It is a commonly used weighting metric in text mining. The formula for TFIDF is:

$$
TFIDF(w,d)=tf(w,d)IDF(w)
$$

where `tf(w,d)` is the term frequency; `IDF(w)` is the inverse document frequency.

# Bibliography

[R-Data-Analysis-Projects: R Data Analysis Projects, published by Packt](https://github.com/PacktPublishing/R-Data-Analysis-Projects)

[Sampling With Replacement vs. Without Replacement](https://www.statology.org/sampling-with-vs-without-replacement)

[How to Normalize Data in R - Statology](https://www.statology.org/how-to-normalize-data-in-r/)
